{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2784259b",
   "metadata": {},
   "source": [
    "Долгодворова Маша, ДЗ№1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbda60",
   "metadata": {},
   "source": [
    "# Датасет с ключевыми словами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c6b20",
   "metadata": {},
   "source": [
    "- Были взяты данные из репозитория к статье SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles https://github.com/snkim/AutomaticKeyphraseExtraction\n",
    "- Из имеющихся датасетов с выделенными ключевыми словами выбран следующий: https://github.com/snkim/AutomaticKeyphraseExtraction/blob/master/Nguyen2007.zip - внутри которого были выбраны 3 текста \n",
    "- Ключевые фразы хранятся в отдельном файле .key\n",
    "- В результате получился csv файл вида text_id | text | keywords | my_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4961706",
   "metadata": {},
   "source": [
    "# 1. Подготовка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd094b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import RAKE\n",
    "from summa import keywords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation, ascii_lowercase, digits\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c2c85",
   "metadata": {},
   "source": [
    "1. Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacdfba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>my_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Entropy and Self-Organization in Multi-Agent S...</td>\n",
       "      <td>self-organization, pheromones, entropy</td>\n",
       "      <td>multi-agent systems, self-organization, pherom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Diagnosis of TCP Overlay Connection Failures u...</td>\n",
       "      <td>bayesian networks, fault diagnosis, passive di...</td>\n",
       "      <td>codeen, bayesian networks, fault diagnosis, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Efficient Multi-way Text Categorization via Ge...</td>\n",
       "      <td>multi-class text categorization, gsvd, discrim...</td>\n",
       "      <td>lsi, multi-class text categorization, gsvd, di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  \\\n",
       "0        1  Entropy and Self-Organization in Multi-Agent S...   \n",
       "1        2  Diagnosis of TCP Overlay Connection Failures u...   \n",
       "2        3  Efficient Multi-way Text Categorization via Ge...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0             self-organization, pheromones, entropy   \n",
       "1  bayesian networks, fault diagnosis, passive di...   \n",
       "2  multi-class text categorization, gsvd, discrim...   \n",
       "\n",
       "                                         my_keywords  \n",
       "0  multi-agent systems, self-organization, pherom...  \n",
       "1  codeen, bayesian networks, fault diagnosis, pa...  \n",
       "2  lsi, multi-class text categorization, gsvd, di...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nlp_keys.csv')\n",
    "df.columns = ['text_id', 'text', 'keywords', 'my_keywords']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad7a12",
   "metadata": {},
   "source": [
    "2. Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5db769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    На вход подается считанный файл\n",
    "    Приведение к одному регистру, удаление пунктуации и стоп-слов, лемматизация\n",
    "    Возвращает леммы \n",
    "    \"\"\"\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if not set(token).intersection(digits)\n",
    "              and token != ', ' and token != \" \" and token not in punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6846c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# некоторые вещи легче убрать так, чем в препроцессинге\n",
    "df['text'] = df.text.replace({'\\n':''}, regex=True).replace({'[()]':''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a523a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6d7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "corpus = []\n",
    "for t in texts:\n",
    "    res = preprocess_text(t)\n",
    "    l += len(res)\n",
    "    corpus.append(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cb9cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Получившийся объем корпуса: 16095 токенов \n"
     ]
    }
   ],
   "source": [
    "print(f\" Получившийся объем корпуса: {l} токенов \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdf897",
   "metadata": {},
   "source": [
    "# Разметка ключевых слов и анализ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fddcd",
   "metadata": {},
   "source": [
    "При выделении ключевых слов (представленных в my_keywords) я была полностью согласна с выделенными ключевыми словами, однако для каждого текста мне показалось стоящим добавить несколько ключевых слов - в результате мои ключевые слова это объединение с исходной разметкой\n",
    "Добавлять слишком много ключевых слов мне кажется не совсем верным - такой подход не помогает в решении задач NLP Например, в задаче извлечении информации:\n",
    "- [*Роль общей и специфической лексики при извлечении информации из текста на примере анализа события «ввод новых технологий»* Юлия Сергеевна Акинина; Анастасия Александровна Бонч-Осмоловская; Илья Олегович Кузнецов; Виктор Петрович Клинцов; Светлана Юрьевна Толдова] https://lib.nsu.ru/xmlui/handle/nsu/257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7579f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = df['my_keywords'].tolist() # все ключевые слова (мои + изначальные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57579adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_keys = df['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e54d02",
   "metadata": {},
   "source": [
    "1. Анализ ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7ebcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначально в 1 тексте выделено ключевых слов: 3 \n",
      "Добавлено при ручной разметке ключевых слов: 2\n",
      "Всего ключевых слов: 5\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Изначально в 2 тексте выделено ключевых слов: 5 \n",
      "Добавлено при ручной разметке ключевых слов: 2\n",
      "Всего ключевых слов: 7\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Изначально в 3 тексте выделено ключевых слов: 3 \n",
      "Добавлено при ручной разметке ключевых слов: 1\n",
      "Всего ключевых слов: 4\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for k in my_keys:\n",
    "    all_k = k.split(', ')\n",
    "    origin_k = origin_keys[s].split(', ')\n",
    "    intersection = list(set(all_k) & set(origin_k))\n",
    "    s+=1\n",
    "    print(f\"Изначально в {s} тексте выделено ключевых слов: {len(origin_k)} \")\n",
    "    print(f\"Добавлено при ручной разметке ключевых слов: {len(all_k) - len(origin_k)}\")\n",
    "    print(f\"Всего ключевых слов: {len(all_k)}\")\n",
    "    print('- - - - - - - - - - - - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ad885",
   "metadata": {},
   "source": [
    "# 3 метода извлечения ключевых слов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2dd21e",
   "metadata": {},
   "source": [
    "Я выбрала RAKE, Tf-Idf, TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9781665",
   "metadata": {},
   "source": [
    "1. Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "tf_key = []\n",
    "\n",
    "tf_keywords = tf_vectorizer.fit_transform(corpus)\n",
    "words = np.array(tf_vectorizer.get_feature_names())\n",
    "arg = np.argsort(tf_keywords.toarray())\n",
    "\n",
    "for i in arg:\n",
    "    index = i[::-1]\n",
    "    tf_key.append(words[index][:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c404c",
   "metadata": {},
   "source": [
    "2. Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4a8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w = stopwords.words('english')\n",
    "rake = RAKE.Rake(stop_w)\n",
    "rake_key = []\n",
    "for i in corpus:\n",
    "    k = rake.run(i, maxWords=6, minFrequency=2)\n",
    "    rake_key.append([i[0] for i in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5978fb6",
   "metadata": {},
   "source": [
    "3. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47079178",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_key = []\n",
    "for i in corpus:\n",
    "    tr_key.append(keywords.keywords(i, language='english', additional_stopwords=stop_w).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dac064",
   "metadata": {},
   "source": [
    "# Применение морфологических шаблонов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3b980",
   "metadata": {},
   "source": [
    "Отфильтруем выделенные автоматически ключевые слова с помощью добавления составленных паттернов для ключевых слов в эталонной разметке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738cce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_morph(keywords):\n",
    "    patterns = ['NN ', 'NNS ', 'JJ NNS', 'JJ NN',\n",
    "               'NNP MD VB NN ', 'NNP NNP NNP ', 'DT']\n",
    "    pos_tags = []\n",
    "    for i in keywords:\n",
    "        res = ''\n",
    "        out = []\n",
    "        for j in nltk.pos_tag(i):\n",
    "            res += (j[1])\n",
    "            res += ' '\n",
    "           \n",
    "            if res in patterns:\n",
    "                out.append(j[0])\n",
    "        pos_tags.append(out)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b048b104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_rake = add_morph(rake_key)\n",
    "m_tr = add_morph(tr_key)\n",
    "m_tf = add_morph(tf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377cb39",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fff4ca",
   "metadata": {},
   "source": [
    "Оценим точность, полноту, F-меру выбранных методов относительно эталона: с учётом морфосинтаксических шаблонов и без них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0507c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "def compute_scores(keywords):\n",
    "    keywords = mlb.transform(keywords)\n",
    "    print(f\"F-score: {f1_score(alls, keywords, average='micro')}\")\n",
    "    print(f\"Precision score: {precision_score(alls, keywords, average='micro')}\")\n",
    "    print(f\"Recall score: {recall_score(alls, keywords, average='micro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b677026",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_k = []\n",
    "for i in df['my_keywords'].tolist() :\n",
    "    m_k.append(i.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6be73213",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = m_k + rake_key + tf_key  + tr_key\n",
    "mlb.fit(all_tags)\n",
    "alls = mlb.transform(m_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ac9acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake\n",
      "F-score: 0.024096385542168672\n",
      "Precision score: 0.012531328320802004\n",
      "Recall score: 0.3125\n",
      "\n",
      "Rake with POS\n",
      "F-score: 0.0\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Tf_idf\n",
      "F-score: 0.04347826086956522\n",
      "Precision score: 0.03333333333333333\n",
      "Recall score: 0.0625\n",
      "\n",
      "Tf_idf with POS\n",
      "F-score: 0.0\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "TextRank\n",
      "F-score: 0.012684989429175475\n",
      "Precision score: 0.0064516129032258064\n",
      "Recall score: 0.375\n",
      "\n",
      "TextRank with POS\n",
      "F-score: 0.10526315789473684\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariadolgodvorova/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Rake')\n",
    "compute_scores(rake_key)\n",
    "print('')\n",
    "print('Rake with POS')\n",
    "compute_scores(m_rake)\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "print('Tf_idf')\n",
    "compute_scores(tf_key)\n",
    "print('')\n",
    "print('Tf_idf with POS')\n",
    "compute_scores(m_tf)\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "print('TextRank')\n",
    "compute_scores(tr_key)\n",
    "print('')\n",
    "print('TextRank with POS')\n",
    "compute_scores(m_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95414625",
   "metadata": {},
   "source": [
    "# Анализ ошибок автоматического выделения ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349d89a",
   "metadata": {},
   "source": [
    "Лучше всего с задачей справился Text-Rank с Pos-тегами, а добавление Pos-тегов в остальных случаях только ухудшило результаты\n",
    "\n",
    "**Анализ результатов**\n",
    "\n",
    "- Во-первых, низкие метрики качества связаны с тем, что методы выделяют намного больше слов, чем в эталонной разметке\n",
    "- Во-вторых, сами корпус - распознанные PDF, и при этом не очищенные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ec3a2",
   "metadata": {},
   "source": [
    "TextRank с Pos-тегами показал лучшие результаты за счет фильтрации лишних слов "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
